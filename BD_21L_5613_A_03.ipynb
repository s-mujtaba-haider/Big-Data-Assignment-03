{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqY1X77glBVL",
        "outputId": "8f42f9de-4fc7-4a91-e903-4dbfecd126cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=689b9696f6c7767d0f3db00376b3a9d8ff7581e49ec3dce354e669e2f855f096\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import random\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "conf = SparkConf().setAppName('A_03').setMaster('local[*]')\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "hnIOuhQYlSDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q - 1\n",
        "spark = SparkSession.builder.appName(\"A_03_Q_1\").getOrCreate()\n",
        "Sentences = spark.read.text(\"InputSentences.txt\").rdd.map(lambda r: r[0])\n",
        "\n",
        "Sentences_ = Sentences.map(lambda word: word.upper())\n",
        "\n",
        "def threelongconsecutivewords(line):\n",
        "    words = line.split()\n",
        "    result = []\n",
        "    for i in range(len(words) - 2):\n",
        "        if (len(words[i]) > 4) and (len(words[i+1]) >= 4) and (len(words[i+2]) >= 4) and (words[i][0] == words[i+1][0] == words[i+2][0]):\n",
        "            result.append(words[i] + \" \" + words[i+1] + \" \" + words[i+2])\n",
        "    return result\n",
        "\n",
        "long_words = Sentences_.flatMap(threelongconsecutivewords)\n",
        "\n",
        "word_counts = long_words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "word = word_counts.map(lambda x:x[1]).sum()\n",
        "\n",
        "alphabet_count = long_words.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
        "alpha_count =  alphabet_count.map(lambda x: (x[0][0], x[1])).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"Total count:\", word)\n",
        "print()\n",
        "for letter, count in alpha_count.collect():\n",
        "    print(letter, \" => \",count)\n",
        "print()\n",
        "for word, count in word_counts.collect():\n",
        "    print(word,\" => \",count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GctoY8i48Dtx",
        "outputId": "2d222ba3-f971-4451-a4d3-fe30d17ea6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total count: 7\n",
            "\n",
            "H  =>  3\n",
            "M  =>  1\n",
            "S  =>  1\n",
            "D  =>  1\n",
            "F  =>  1\n",
            "\n",
            "HORRID HENRY’S HOUND  =>  2\n",
            "HENRY’S HOUND HUNTS  =>  1\n",
            "MASSIVE MURREE MOUNTAINS.  =>  1\n",
            "SILLY STUPID SAMUEL’S  =>  1\n",
            "DREADFUL DRAGON DANY  =>  1\n",
            "FANTASTIC FANCIFUL FOURSOME.  =>  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q - 2\n",
        "input_file = sc.textFile(\"InputFile.txt\")\n",
        "input_data = input_file.map(lambda line: line.split(\":\"))\n",
        "input_data = input_data.map(lambda x: (x[0], list(map(int, x[1].split()))))\n",
        "\n",
        "reference_file = sc.textFile(\"Reference.txt\")\n",
        "reference_data = reference_file.map(lambda line: line.split(\":\"))\n",
        "reference_data = reference_data.map(lambda x: (x[0], list(map(int, x[1].split()))))\n",
        "\n",
        "def jaccard_distance(a, b):\n",
        "    intersection = sum([1 for i in range(len(a)) if a[i] == 1 and b[i] == 1])\n",
        "    union = sum([1 for i in range(len(a)) if a[i] == 1 or b[i] == 1])\n",
        "    return intersection / float(union)\n",
        "\n",
        "input_ref_join = input_data.cartesian(reference_data)\n",
        "input_ref_join = input_ref_join.map(lambda x: (x[0][0], x[1][0], jaccard_distance(x[0][1], x[1][1])))\n",
        "\n",
        "closest_ref = input_ref_join.groupBy(lambda x: x[0]).map(lambda x: (x[0], max(x[1], key=lambda y: y[2])[1]))\n",
        "\n",
        "for element in sorted(closest_ref.collect()):\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "WnhjqbXylne-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1378367d-79f7-430d-cf8d-5c9df1b48bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('1', 'R1')\n",
            "('2', 'R1')\n",
            "('3', 'R1')\n",
            "('4', 'R2')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q - 3\n",
        "input_data = sc.textFile(\"InputAuthor.txt\")\n",
        "authors = input_data.map(lambda line: (line.split(\"->\")[0].strip(), line.split(\"->\")[1].strip().split(\", \")))\n",
        "\n",
        "common_coauthors = authors.cartesian(authors).filter(lambda pair: pair[0][0] < pair[1][0])\n",
        "cmn_coauthors = common_coauthors.flatMap(lambda pair: [((pair[0][0], pair[1][0]), coauthor) for coauthor in pair[0][1] if coauthor in pair[1][1]])\n",
        "grouping_coauthors = cmn_coauthors.groupByKey()\n",
        "sorted_authors = grouping_coauthors.mapValues(lambda coauthors: (len(coauthors), sorted(coauthors)))\n",
        "coauthors = sorted_authors.filter(lambda pair: pair[1][0] > 0)\n",
        "\n",
        "for pair, (count, cauthors) in coauthors.collect():\n",
        "    print(pair[0] + \", \" + pair[1] + \" -> (\" + str(count) + \") \" + \", \".join(cauthors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KwkPcxZA-Co",
        "outputId": "df95bc0d-89f5-46f9-8e35-d7f3d748274e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B. Cao, C. Rego -> (1) Y. Lu\n",
            "B. Cao, K. Kiani -> (1) Y. Lu\n",
            "C. Rego, K. Kiani -> (2) B. Cao, Y. Lu\n",
            "B. Cao, Y. Lu -> (3) C. Rego, F. Glover, K. Kiani\n",
            "C. Rego, Y. Lu -> (1) B. Cao\n",
            "B. Cao, F. Glover -> (1) Y. Lu\n",
            "C. Rego, F. Glover -> (2) B. Cao, Y. Lu\n",
            "B. Cao, B. Hosseini -> (1) K. Kiani\n",
            "K. Kiani, Y. Lu -> (1) B. Cao\n",
            "F. Glover, Y. Lu -> (1) B. Cao\n",
            "B. Hosseini, Y. Lu -> (1) K. Kiani\n",
            "F. Glover, K. Kiani -> (2) B. Cao, Y. Lu\n"
          ]
        }
      ]
    }
  ]
}